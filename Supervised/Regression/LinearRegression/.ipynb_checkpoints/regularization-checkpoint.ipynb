{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "## Cost Function\n",
    "Let's assume we have two functions: $\\Theta_0 +\\Theta_1x + \\Theta_2x^2$ (that fits well the data) and $\\Theta_0 +\\Theta_1x + \\Theta_2x^2 + \\Theta_3x^3 + \\Theta_4x^4$ (that overfits the data). How can we penalize the second function to avoid overfitting?\n",
    "\n",
    "Remembering that the cost function is: $J = \\frac{1}{2m}\\sum_{i=1}^m(h_{\\Theta}(x^{(i)}) - y^{(i)})^2$\n",
    "\n",
    "Ex: We add terms $1000*\\Theta_3^3$ and $1000*\\Theta_4^4$ to the cost function in order to make $\\Theta_3$ and $\\Theta_4$ small.\n",
    "\n",
    "## Regularized Cost Function\n",
    "$J = \\frac{1}{2m} [\\sum_{i=1}^m(h_{\\Theta}(x^{(i)}) - y^{(i)})^2 + \\lambda\\sum_{j=1}^n\\Theta_j^2]$.\n",
    "\n",
    "The regularized term is added in order to keep the parameters small. \n",
    "\n",
    "### Regularization parameter\n",
    "The regularization parameter $\\lambda$ controls the trade off between the goal of fitting the data and keeping the parameters small (keeping the hypothesis simple).\n",
    "\n",
    "It is important to note that if $\\lambda$ is too large (say $10^10$), the algorithm might result in underfitting.\n",
    "\n",
    "## Regularized Linear Regression\n",
    "\n",
    "$\\Theta_j = \\Theta_j - \\alpha[\\frac{1}{m}\\sum^m_{i=1}(h_{\\Theta}(x^i)-y_i)*x^i_j + \\frac{\\lambda}{m}\\Theta_j]$\n",
    "\n",
    "OBS: We keep the same function for $\\Theta_0$ (we don't want to penalize it)\n",
    "\n",
    "The above can also be written as:\n",
    "$\\Theta_j = \\Theta_j(1 - \\alpha\\frac{\\lambda}{m})- \\alpha\\frac{1}{m}\\sum^m_{i=1}(h_{\\Theta}(x^i)-y_i)*x^i_j$\n",
    "\n",
    "Note that $(1 - \\alpha\\frac{\\lambda}{m})$ will always be less than one.\n",
    "\n",
    "<hr style=\"border:2px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "Add the term $\\lambda\\sum_{j=1}^n\\Theta_j^2$ to the cost function, forcing the algorithm to **keep the model weights as small as possible**.\n",
    "\n",
    "This term should only be added to the cost function during **training**.\n",
    "\n",
    "## L2 Regularization\n",
    "Src: https://towardsdatascience.com/intuitions-on-l1-and-l2-regularisation-235f2db4c261\n",
    "\n",
    "L1 and L2 regularization are named after the L1 and L2 norm of a vector $w$.\n",
    "\n",
    "L2 norm: $\\left\\| w \\right\\|_2 = (|w_1|^2 + |w_2|^2 + ... + |w_n|^2)^{\\frac{1}{2}}$\n",
    "\n",
    "Therefore, Ridge Regression uses a loss function with **squared L2 norm of the weights** (notice the absence of the square root)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.57274367]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
    "ridge_reg = Ridge(alpha=1, solver=\"cholesky\")\n",
    "ridge_reg.fit(X, y)\n",
    "ridge_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent\n",
    "\n",
    "The penalty hyperparameter sets the type of regularization term to use. Specifying \"l2\" indicates that you want SGD to add a regularization term to the cost function equal to half the square of the $l2$ norm of the weight vector: this is simply Ridge\n",
    "Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.55364118])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(penalty=\"l2\")\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "sgd_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression\n",
    "\n",
    "Add the term $\\lambda\\sum_{j=1}^n|\\Theta_j|$ to the cost function.\n",
    "\n",
    "## L1 Regularization\n",
    "Lasso Regression uses the $l1$ norm of the weight vector instead of half the square of the $l2$ norm.\n",
    "\n",
    "L1 norm: $\\left\\| w \\right\\|_2 = (|w_1| + |w_2| + ... + |w_n|)$\n",
    "\n",
    "## Difference from Ridge\n",
    "Lasso shrinks the less important featureâ€™s coefficient to zero and so it may **remove some features** altogether. This works well for **feature selection** in case we have a huge number of features.\n",
    "\n",
    "https://www.youtube.com/watch?v=Xm2C_gTAl8c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.09245718])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
    "lasso_reg = Lasso(alpha=0.1)\n",
    "lasso_reg.fit(X, y)\n",
    "lasso_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.10832161])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "sgd_reg = SGDRegressor(penalty=\"l1\")\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "sgd_reg.predict([[1.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early Stopping\n",
    "Stop training as soon as the validation error reaches a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "sgd_reg = SGDRegressor(n_iter=1, warm_start=True, penalty=None,\n",
    " learning_rate=\"constant\", eta0=0.0005)\n",
    "minimum_val_error = float(\"inf\")\n",
    "best_epoch = None\n",
    "best_model = None\n",
    "for epoch in range(1000):\n",
    "    sgd_reg.fit(X_train_poly_scaled, y_train) # continues where it left off\n",
    "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
    "    val_error = mean_squared_error(y_val_predict, y_val)\n",
    "    if val_error < minimum_val_error:\n",
    "        minimum_val_error = val_error\n",
    "        best_epoch = epoch\n",
    "        best_model = clone(sgd_reg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
