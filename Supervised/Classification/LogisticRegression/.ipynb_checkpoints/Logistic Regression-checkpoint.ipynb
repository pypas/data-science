{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "Logistic Regression is used when the dependent variable(target) is **categorical**.\n",
    "\n",
    "The hypothesis function is $h_{\\Theta}(x) = \\Theta^Tx$.\n",
    "- if $h_{\\Theta} \\geq 0.5$, predict $y = 1$\n",
    "\n",
    "- if $h_{\\Theta} \\leq 0.5$, predict $y = 0$\n",
    "\n",
    "Since we want $0 \\leq h_{\\Theta} \\leq 1$: $h_{\\Theta}(x) = g(\\Theta^Tx)$. \n",
    "\n",
    "The function $g(z) = \\frac{1}{1+e^{-z}}$ is called the **sigmoid function** or **logistic function**\n",
    "\n",
    "We can interpret $h_{\\Theta}(x)$ as the estimate probability that $y=1$ on input $x$, that is, $P(y=1|x;\\Theta)$.\n",
    "\n",
    "## Decision boundary\n",
    "A decision boundary is the region of a problem space in which the output label of a classifier is ambiguous.\n",
    "\n",
    "## Cost function\n",
    "Src: https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html\n",
    "\\+ http://neuralnetworksanddeeplearning.com/chap3.html\n",
    "\n",
    "We want to minimize the cost function $J(\\Theta) = \\frac{1}{2m}\\sum_{i=1}^m (h_{\\Theta}(x^i) - y^i)^2 $.\n",
    "\n",
    "This time, we cannot use the same loss function as before (Mean Squared Error), since the **prediction function is non-linear**. Squaring this prediction as we do in MSE results in a non-convex function with many local minimums (which means gradient descent may not find the optimal global minimum)\n",
    "\n",
    "### Cross-Entropy\n",
    "$J(\\Theta) = \\frac{1}{m}\\sum_{i=1}^m Cost(h_{\\Theta}(x^i),y^i)$, where\n",
    "\n",
    "- $Cost(h_{\\Theta}(x^i),y^i) = -log(h_{\\Theta}(x))$, when $y = 1$\n",
    "\n",
    "- $Cost(h_{\\Theta}(x^i),y^i) = -log(1 - h_{\\Theta}(x))$, when $y = 0$\n",
    "\n",
    "Cost = 0 if $y = 1$ and $h_{\\Theta}(x) = 1$, but as $h_{\\Theta}(x) -> 1$, $Cost -> \\infty $  \n",
    "\n",
    "The cost function **penalizes confident and wrong** predictions more than it rewards confident and right predictions.\n",
    "\n",
    "### Simpllified Cost Function\n",
    "$Cost(h_{\\Theta}(x^i),y^i) = -ylog(h_{\\Theta}(x)) -(1-y)log(1 - h_{\\Theta}(x))$\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "### The algorithm\n",
    "$\\Theta_j = \\Theta_j - \\alpha\\frac{\\delta}{\\delta \\Theta_j}J(\\Theta)$\n",
    "\n",
    "### Replacing the Cost Function\n",
    "$\\Theta_j = \\Theta_j - \\alpha\\sum_{i=1}^m(h_{\\Theta}(x^i) - y^i)*x^i_j$\n",
    "\n",
    "## Multiclass classification\n",
    "\n",
    "### One vs All (One vs Rest)\n",
    "Train a logistic regression classifier $h_{\\Theta}^i(x)$ for each class $i$ to predict the probability that $y = i$. We train a single classifier per class, with the samples of that class as positive samples and all other samples as negatives. While testing, you simply classify the sample as belonging to the class with maximum score among the N classifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
