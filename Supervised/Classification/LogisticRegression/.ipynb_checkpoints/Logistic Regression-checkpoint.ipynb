{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "Logistic Regression is used when the dependent variable(target) is **categorical**.\n",
    "\n",
    "The hypothesis function is $h_{\\Theta}(x) = \\Theta^Tx$.\n",
    "- if $h_{\\Theta} \\geq 0.5$, predict $y = 1$\n",
    "\n",
    "- if $h_{\\Theta} \\leq 0.5$, predict $y = 0$\n",
    "\n",
    "Since we want $0 \\leq h_{\\Theta} \\leq 1$: $h_{\\Theta}(x) = g(\\Theta^Tx)$. \n",
    "\n",
    "The function $g(z) = \\frac{1}{1+e^{-z}}$ is called the **sigmoid function** or **logistic function**\n",
    "\n",
    "We can interpret $h_{\\Theta}(x)$ as the estimate probability that $y=1$ on input $x$, that is, $P(y=1|x;\\Theta)$.\n",
    "\n",
    "## Why not Linear Regression?\n",
    "Src: https://jinglescode.github.io/2019/05/07/why-linear-regression-is-not-suitable-for-classification/\n",
    "\n",
    "## Decision boundary\n",
    "A decision boundary is the region of a problem space in which the output label of a classifier is ambiguous.\n",
    "\n",
    "## Cost function\n",
    "Src: https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html\n",
    "\\+ http://neuralnetworksanddeeplearning.com/chap3.html\n",
    "\n",
    "We want to minimize the cost function $J(\\Theta) = \\frac{1}{2m}\\sum_{i=1}^m (h_{\\Theta}(x^i) - y^i)^2 $.\n",
    "\n",
    "This time, we cannot use the same loss function as before (Mean Squared Error), since the **prediction function is non-linear** (due to the sigmoid transformation). Squaring this prediction as we do in MSE results in a non-convex function with many local minimums (which means gradient descent may not find the optimal global minimum)\n",
    "\n",
    "### Cross-Entropy\n",
    "$J(\\Theta) = \\frac{1}{m}\\sum_{i=1}^m Cost(h_{\\Theta}(x^i),y^i)$, where\n",
    "\n",
    "- $Cost(h_{\\Theta}(x^i),y^i) = -log(h_{\\Theta}(x))$, when $y = 1$\n",
    "\n",
    "- $Cost(h_{\\Theta}(x^i),y^i) = -log(1 - h_{\\Theta}(x))$, when $y = 0$\n",
    "\n",
    "![logistic](img/logistic_function.png)\n",
    "\n",
    "Cost = 0 if $y = 1$ and $h_{\\Theta}(x) = 1$, but as $h_{\\Theta}(x) -> 0$, $Cost -> \\infty $  \n",
    "\n",
    "The cost function **penalizes confident and wrong** predictions more than it rewards confident and right predictions.\n",
    "\n",
    "### Simpllified Cost Function\n",
    "$Cost(h_{\\Theta}(x^i),y^i) = -ylog(h_{\\Theta}(x)) -(1-y)log(1 - h_{\\Theta}(x))$\n",
    "\n",
    "### Log Loss\n",
    "Log-loss = $- \\frac{1}{n}\\sum_{i=1}^n[y_ilog(\\hat{y_i}) + (1-y_i)log(1 - \\hat{y_i})]$\n",
    "\n",
    "Derivative = $-\\frac{1}{n}(y_i-\\hat{y_i})x_i$\n",
    "\n",
    "The derivative is proportional to the difference between the label and the predicted value. If the prediction is good, the derivative is low (nice!).\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "To minimize the cost, we can use gradient descent.\n",
    "\n",
    "### The algorithm\n",
    "$\\Theta_j = \\Theta_j - \\alpha\\frac{\\delta}{\\delta \\Theta_j}J(\\Theta)$\n",
    "\n",
    "### Replacing the Cost Function\n",
    "$\\Theta_j = \\Theta_j - \\alpha\\sum_{i=1}^m(h_{\\Theta}(x^i) - y^i)*x^i_j$\n",
    "\n",
    "## Multiclass classification\n",
    "\n",
    "### One vs All (One vs Rest)\n",
    "Train a logistic regression classifier $h_{\\Theta}^i(x)$ for each class $i$ to predict the probability that $y = i$. We train a single classifier per class, with the samples of that class as positive samples and all other samples as negatives. While testing, you simply classify the sample as belonging to the class with maximum score among the N classifiers.\n",
    "\n",
    "## Regularization\n",
    "Regularization is extremely important in logistic regression modeling. Without regularization, **the asymptotic nature of logistic regression would keep driving loss towards 0 in high dimensions**. Consequently, most logistic regression models use one of the following two strategies to dampen model complexity:\n",
    "\n",
    "- L2 regularization.\n",
    "- Early stopping, that is, limiting the number of training steps or the learning rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>sp.ent</th>\n",
       "      <th>sfm</th>\n",
       "      <th>...</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.059781</td>\n",
       "      <td>0.064241</td>\n",
       "      <td>0.032027</td>\n",
       "      <td>0.015071</td>\n",
       "      <td>0.090193</td>\n",
       "      <td>0.075122</td>\n",
       "      <td>12.863462</td>\n",
       "      <td>274.402906</td>\n",
       "      <td>0.893369</td>\n",
       "      <td>0.491918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059781</td>\n",
       "      <td>0.084279</td>\n",
       "      <td>0.015702</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.066009</td>\n",
       "      <td>0.067310</td>\n",
       "      <td>0.040229</td>\n",
       "      <td>0.019414</td>\n",
       "      <td>0.092666</td>\n",
       "      <td>0.073252</td>\n",
       "      <td>22.423285</td>\n",
       "      <td>634.613855</td>\n",
       "      <td>0.892193</td>\n",
       "      <td>0.513724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066009</td>\n",
       "      <td>0.107937</td>\n",
       "      <td>0.015826</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.009014</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.077316</td>\n",
       "      <td>0.083829</td>\n",
       "      <td>0.036718</td>\n",
       "      <td>0.008701</td>\n",
       "      <td>0.131908</td>\n",
       "      <td>0.123207</td>\n",
       "      <td>30.757155</td>\n",
       "      <td>1024.927705</td>\n",
       "      <td>0.846389</td>\n",
       "      <td>0.478905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077316</td>\n",
       "      <td>0.098706</td>\n",
       "      <td>0.015656</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.007990</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.072111</td>\n",
       "      <td>0.158011</td>\n",
       "      <td>0.096582</td>\n",
       "      <td>0.207955</td>\n",
       "      <td>0.111374</td>\n",
       "      <td>1.232831</td>\n",
       "      <td>4.177296</td>\n",
       "      <td>0.963322</td>\n",
       "      <td>0.727232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.017798</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.201497</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.247119</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.135120</td>\n",
       "      <td>0.079146</td>\n",
       "      <td>0.124656</td>\n",
       "      <td>0.078720</td>\n",
       "      <td>0.206045</td>\n",
       "      <td>0.127325</td>\n",
       "      <td>1.101174</td>\n",
       "      <td>4.333713</td>\n",
       "      <td>0.971955</td>\n",
       "      <td>0.783568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135120</td>\n",
       "      <td>0.106398</td>\n",
       "      <td>0.016931</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.712812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>5.484375</td>\n",
       "      <td>5.476562</td>\n",
       "      <td>0.208274</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3163</th>\n",
       "      <td>0.131884</td>\n",
       "      <td>0.084734</td>\n",
       "      <td>0.153707</td>\n",
       "      <td>0.049285</td>\n",
       "      <td>0.201144</td>\n",
       "      <td>0.151859</td>\n",
       "      <td>1.762129</td>\n",
       "      <td>6.630383</td>\n",
       "      <td>0.962934</td>\n",
       "      <td>0.763182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131884</td>\n",
       "      <td>0.182790</td>\n",
       "      <td>0.083770</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.832899</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>4.210938</td>\n",
       "      <td>4.203125</td>\n",
       "      <td>0.161929</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3164</th>\n",
       "      <td>0.116221</td>\n",
       "      <td>0.089221</td>\n",
       "      <td>0.076758</td>\n",
       "      <td>0.042718</td>\n",
       "      <td>0.204911</td>\n",
       "      <td>0.162193</td>\n",
       "      <td>0.693730</td>\n",
       "      <td>2.503954</td>\n",
       "      <td>0.960716</td>\n",
       "      <td>0.709570</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116221</td>\n",
       "      <td>0.188980</td>\n",
       "      <td>0.034409</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.909856</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>3.679688</td>\n",
       "      <td>3.640625</td>\n",
       "      <td>0.277897</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3165</th>\n",
       "      <td>0.142056</td>\n",
       "      <td>0.095798</td>\n",
       "      <td>0.183731</td>\n",
       "      <td>0.033424</td>\n",
       "      <td>0.224360</td>\n",
       "      <td>0.190936</td>\n",
       "      <td>1.876502</td>\n",
       "      <td>6.604509</td>\n",
       "      <td>0.946854</td>\n",
       "      <td>0.654196</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142056</td>\n",
       "      <td>0.209918</td>\n",
       "      <td>0.039506</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.494271</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>2.937500</td>\n",
       "      <td>2.929688</td>\n",
       "      <td>0.194759</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3166</th>\n",
       "      <td>0.143659</td>\n",
       "      <td>0.090628</td>\n",
       "      <td>0.184976</td>\n",
       "      <td>0.043508</td>\n",
       "      <td>0.219943</td>\n",
       "      <td>0.176435</td>\n",
       "      <td>1.591065</td>\n",
       "      <td>5.388298</td>\n",
       "      <td>0.950436</td>\n",
       "      <td>0.675470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143659</td>\n",
       "      <td>0.172375</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.791360</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>3.593750</td>\n",
       "      <td>3.585938</td>\n",
       "      <td>0.311002</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3167</th>\n",
       "      <td>0.165509</td>\n",
       "      <td>0.092884</td>\n",
       "      <td>0.183044</td>\n",
       "      <td>0.070072</td>\n",
       "      <td>0.250827</td>\n",
       "      <td>0.180756</td>\n",
       "      <td>1.705029</td>\n",
       "      <td>5.769115</td>\n",
       "      <td>0.938829</td>\n",
       "      <td>0.601529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165509</td>\n",
       "      <td>0.185607</td>\n",
       "      <td>0.062257</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.227022</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3168 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      meanfreq        sd    median       Q25       Q75       IQR       skew  \\\n",
       "0     0.059781  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n",
       "1     0.066009  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n",
       "2     0.077316  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n",
       "3     0.151228  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n",
       "4     0.135120  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n",
       "...        ...       ...       ...       ...       ...       ...        ...   \n",
       "3163  0.131884  0.084734  0.153707  0.049285  0.201144  0.151859   1.762129   \n",
       "3164  0.116221  0.089221  0.076758  0.042718  0.204911  0.162193   0.693730   \n",
       "3165  0.142056  0.095798  0.183731  0.033424  0.224360  0.190936   1.876502   \n",
       "3166  0.143659  0.090628  0.184976  0.043508  0.219943  0.176435   1.591065   \n",
       "3167  0.165509  0.092884  0.183044  0.070072  0.250827  0.180756   1.705029   \n",
       "\n",
       "             kurt    sp.ent       sfm  ...  centroid   meanfun    minfun  \\\n",
       "0      274.402906  0.893369  0.491918  ...  0.059781  0.084279  0.015702   \n",
       "1      634.613855  0.892193  0.513724  ...  0.066009  0.107937  0.015826   \n",
       "2     1024.927705  0.846389  0.478905  ...  0.077316  0.098706  0.015656   \n",
       "3        4.177296  0.963322  0.727232  ...  0.151228  0.088965  0.017798   \n",
       "4        4.333713  0.971955  0.783568  ...  0.135120  0.106398  0.016931   \n",
       "...           ...       ...       ...  ...       ...       ...       ...   \n",
       "3163     6.630383  0.962934  0.763182  ...  0.131884  0.182790  0.083770   \n",
       "3164     2.503954  0.960716  0.709570  ...  0.116221  0.188980  0.034409   \n",
       "3165     6.604509  0.946854  0.654196  ...  0.142056  0.209918  0.039506   \n",
       "3166     5.388298  0.950436  0.675470  ...  0.143659  0.172375  0.034483   \n",
       "3167     5.769115  0.938829  0.601529  ...  0.165509  0.185607  0.062257   \n",
       "\n",
       "        maxfun   meandom    mindom    maxdom   dfrange   modindx  label  \n",
       "0     0.275862  0.007812  0.007812  0.007812  0.000000  0.000000      0  \n",
       "1     0.250000  0.009014  0.007812  0.054688  0.046875  0.052632      0  \n",
       "2     0.271186  0.007990  0.007812  0.015625  0.007812  0.046512      0  \n",
       "3     0.250000  0.201497  0.007812  0.562500  0.554688  0.247119      0  \n",
       "4     0.266667  0.712812  0.007812  5.484375  5.476562  0.208274      0  \n",
       "...        ...       ...       ...       ...       ...       ...    ...  \n",
       "3163  0.262295  0.832899  0.007812  4.210938  4.203125  0.161929      1  \n",
       "3164  0.275862  0.909856  0.039062  3.679688  3.640625  0.277897      1  \n",
       "3165  0.275862  0.494271  0.007812  2.937500  2.929688  0.194759      1  \n",
       "3166  0.250000  0.791360  0.007812  3.593750  3.585938  0.311002      1  \n",
       "3167  0.271186  0.227022  0.007812  0.554688  0.546875  0.350000      1  \n",
       "\n",
       "[3168 rows x 21 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('voice.csv')\n",
    "df[\"label\"] = [1 if x =='female' else 0 for x in df.label ]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 0:20].values\n",
    "y = df.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "**StandardScaler** transforms the data such that its distribution will have a mean value 0 and standard deviation of 1.\n",
    "\n",
    "$z = \\frac{(x - \\mu)}{s}$, where $x$ is the sample, $\\mu$ is the mean of the training samples and $s$ is the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression: hard-coded\n",
    "\n",
    "We can think of logistic regression as a **one layer neural network**.\n",
    "\n",
    "### Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def init(dimension):\n",
    "    w = np.full((dimension,1),0.01)\n",
    "    b=0.0\n",
    "    return w,b\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "\n",
    "$z^{(i)} = w^Tx^{(i)} + b$\n",
    "\n",
    "$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})$\n",
    "\n",
    "$Cost(a^{(i)},y^{(i)}) = -y^{(i)}log(a^{(i)}) -(1-y^{(i)})log(1 - a^{(i)})$\n",
    "\n",
    "$J = \\frac{1}{m}\\sum_{i=1}^m Cost(a^{(i)},y^{(i)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward_prop(w,b,X_train,y_train):\n",
    "    m = X_train.shape[1]\n",
    "    # Forward propagation\n",
    "    z = np.dot(w.T,X_train) + b\n",
    "    y_head = sigmoid(z) #compute activation\n",
    "    loss = -(y_train*np.log(y_head)+(1-y_train)*np.log(1-y_head))\n",
    "    cost = np.sum(loss)/m\n",
    "    \n",
    "    # Backward propagation\n",
    "    weight = (np.dot(X_train,(y_head-y_train).T))/m\n",
    "    bias = np.sum(y_head-y_train)/m\n",
    "    \n",
    "    grad = {\"weight\":weight,\"bias\":bias}\n",
    "    \n",
    "    return cost,grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(w,b,X_train,y_train,learning_rate,num):\n",
    "    cost_list=[]\n",
    "    index=[]\n",
    "    for i in range(num):\n",
    "        cost , grad = forward_backward_prop(w,b,X_train,y_train)\n",
    "        w = w - learning_rate*grad[\"weight\"]    \n",
    "        b = b - learning_rate*grad[\"bias\"]\n",
    "        if i % 100 == 0:\n",
    "            cost_list.append(cost)\n",
    "            index.append(i)\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    parameters={\"weight\":w,\"bias\":b}\n",
    "    plt.plot(index,cost_list)\n",
    "    plt.xlabel(\"Num of iteration\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.show()\n",
    "    return parameters,grad,cost_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w,b,X_test):\n",
    "    z = sigmoid(np.dot(w.T,X_test)+b)\n",
    "    prediction = np.zeros((1,X_test.shape[1]))\n",
    "    for i in range(z.shape[1]):\n",
    "        if(z[0,i]<=0.5):\n",
    "            prediction[0,i]=0\n",
    "        else:\n",
    "            prediction[0,i]=1\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X_train,y_train,X_test,y_test,learning_rate,num):\n",
    "    w,b = init(X_train.shape[0])\n",
    "    parameters,grad,cost_list = update(w,b,X_train,y_train,learning_rate,num)\n",
    "    \n",
    "    prediction = predict(parameters[\"weight\"],parameters[\"bias\"],X_test)\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(prediction - y_test)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.686877\n",
      "Cost after iteration 100: 0.197842\n",
      "Cost after iteration 200: 0.148695\n",
      "Cost after iteration 300: 0.129561\n",
      "Cost after iteration 400: 0.119504\n",
      "Cost after iteration 500: 0.113376\n",
      "Cost after iteration 600: 0.109294\n",
      "Cost after iteration 700: 0.106409\n",
      "Cost after iteration 800: 0.104281\n",
      "Cost after iteration 900: 0.102659\n",
      "Cost after iteration 1000: 0.101393\n",
      "Cost after iteration 1100: 0.100383\n",
      "Cost after iteration 1200: 0.099565\n",
      "Cost after iteration 1300: 0.098891\n",
      "Cost after iteration 1400: 0.098331\n",
      "Cost after iteration 1500: 0.097860\n",
      "Cost after iteration 1600: 0.097460\n",
      "Cost after iteration 1700: 0.097118\n",
      "Cost after iteration 1800: 0.096823\n",
      "Cost after iteration 1900: 0.096567\n",
      "Cost after iteration 2000: 0.096344\n",
      "Cost after iteration 2100: 0.096148\n",
      "Cost after iteration 2200: 0.095975\n",
      "Cost after iteration 2300: 0.095822\n",
      "Cost after iteration 2400: 0.095685\n",
      "Cost after iteration 2500: 0.095563\n",
      "Cost after iteration 2600: 0.095453\n",
      "Cost after iteration 2700: 0.095355\n",
      "Cost after iteration 2800: 0.095265\n",
      "Cost after iteration 2900: 0.095184\n",
      "Cost after iteration 3000: 0.095110\n",
      "Cost after iteration 3100: 0.095043\n",
      "Cost after iteration 3200: 0.094981\n",
      "Cost after iteration 3300: 0.094924\n",
      "Cost after iteration 3400: 0.094872\n",
      "Cost after iteration 3500: 0.094824\n",
      "Cost after iteration 3600: 0.094779\n",
      "Cost after iteration 3700: 0.094737\n",
      "Cost after iteration 3800: 0.094698\n",
      "Cost after iteration 3900: 0.094662\n",
      "Cost after iteration 4000: 0.094628\n",
      "Cost after iteration 4100: 0.094597\n",
      "Cost after iteration 4200: 0.094567\n",
      "Cost after iteration 4300: 0.094539\n",
      "Cost after iteration 4400: 0.094512\n",
      "Cost after iteration 4500: 0.094487\n",
      "Cost after iteration 4600: 0.094463\n",
      "Cost after iteration 4700: 0.094440\n",
      "Cost after iteration 4800: 0.094419\n",
      "Cost after iteration 4900: 0.094398\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 97.6010101010101 %\n"
     ]
    }
   ],
   "source": [
    "logistic_regression(X_train.T,y_train.T,X_test.T,y_test.T,learning_rate=0.1,num=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression: sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.9785353535353535\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train,y_train)\n",
    "print(\"test accuracy {}\".format(classifier.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[414,   5],\n",
       "       [ 12, 361]], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>sp.ent</th>\n",
       "      <th>sfm</th>\n",
       "      <th>...</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.059781</td>\n",
       "      <td>0.064241</td>\n",
       "      <td>0.032027</td>\n",
       "      <td>0.015071</td>\n",
       "      <td>0.090193</td>\n",
       "      <td>0.075122</td>\n",
       "      <td>12.863462</td>\n",
       "      <td>274.402906</td>\n",
       "      <td>0.893369</td>\n",
       "      <td>0.491918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059781</td>\n",
       "      <td>0.084279</td>\n",
       "      <td>0.015702</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.066009</td>\n",
       "      <td>0.067310</td>\n",
       "      <td>0.040229</td>\n",
       "      <td>0.019414</td>\n",
       "      <td>0.092666</td>\n",
       "      <td>0.073252</td>\n",
       "      <td>22.423285</td>\n",
       "      <td>634.613855</td>\n",
       "      <td>0.892193</td>\n",
       "      <td>0.513724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066009</td>\n",
       "      <td>0.107937</td>\n",
       "      <td>0.015826</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.009014</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.077316</td>\n",
       "      <td>0.083829</td>\n",
       "      <td>0.036718</td>\n",
       "      <td>0.008701</td>\n",
       "      <td>0.131908</td>\n",
       "      <td>0.123207</td>\n",
       "      <td>30.757155</td>\n",
       "      <td>1024.927705</td>\n",
       "      <td>0.846389</td>\n",
       "      <td>0.478905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077316</td>\n",
       "      <td>0.098706</td>\n",
       "      <td>0.015656</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.007990</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.072111</td>\n",
       "      <td>0.158011</td>\n",
       "      <td>0.096582</td>\n",
       "      <td>0.207955</td>\n",
       "      <td>0.111374</td>\n",
       "      <td>1.232831</td>\n",
       "      <td>4.177296</td>\n",
       "      <td>0.963322</td>\n",
       "      <td>0.727232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.017798</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.201497</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.247119</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.135120</td>\n",
       "      <td>0.079146</td>\n",
       "      <td>0.124656</td>\n",
       "      <td>0.078720</td>\n",
       "      <td>0.206045</td>\n",
       "      <td>0.127325</td>\n",
       "      <td>1.101174</td>\n",
       "      <td>4.333713</td>\n",
       "      <td>0.971955</td>\n",
       "      <td>0.783568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135120</td>\n",
       "      <td>0.106398</td>\n",
       "      <td>0.016931</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.712812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>5.484375</td>\n",
       "      <td>5.476562</td>\n",
       "      <td>0.208274</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3163</th>\n",
       "      <td>0.131884</td>\n",
       "      <td>0.084734</td>\n",
       "      <td>0.153707</td>\n",
       "      <td>0.049285</td>\n",
       "      <td>0.201144</td>\n",
       "      <td>0.151859</td>\n",
       "      <td>1.762129</td>\n",
       "      <td>6.630383</td>\n",
       "      <td>0.962934</td>\n",
       "      <td>0.763182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131884</td>\n",
       "      <td>0.182790</td>\n",
       "      <td>0.083770</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.832899</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>4.210938</td>\n",
       "      <td>4.203125</td>\n",
       "      <td>0.161929</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3164</th>\n",
       "      <td>0.116221</td>\n",
       "      <td>0.089221</td>\n",
       "      <td>0.076758</td>\n",
       "      <td>0.042718</td>\n",
       "      <td>0.204911</td>\n",
       "      <td>0.162193</td>\n",
       "      <td>0.693730</td>\n",
       "      <td>2.503954</td>\n",
       "      <td>0.960716</td>\n",
       "      <td>0.709570</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116221</td>\n",
       "      <td>0.188980</td>\n",
       "      <td>0.034409</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.909856</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>3.679688</td>\n",
       "      <td>3.640625</td>\n",
       "      <td>0.277897</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3165</th>\n",
       "      <td>0.142056</td>\n",
       "      <td>0.095798</td>\n",
       "      <td>0.183731</td>\n",
       "      <td>0.033424</td>\n",
       "      <td>0.224360</td>\n",
       "      <td>0.190936</td>\n",
       "      <td>1.876502</td>\n",
       "      <td>6.604509</td>\n",
       "      <td>0.946854</td>\n",
       "      <td>0.654196</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142056</td>\n",
       "      <td>0.209918</td>\n",
       "      <td>0.039506</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.494271</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>2.937500</td>\n",
       "      <td>2.929688</td>\n",
       "      <td>0.194759</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3166</th>\n",
       "      <td>0.143659</td>\n",
       "      <td>0.090628</td>\n",
       "      <td>0.184976</td>\n",
       "      <td>0.043508</td>\n",
       "      <td>0.219943</td>\n",
       "      <td>0.176435</td>\n",
       "      <td>1.591065</td>\n",
       "      <td>5.388298</td>\n",
       "      <td>0.950436</td>\n",
       "      <td>0.675470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143659</td>\n",
       "      <td>0.172375</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.791360</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>3.593750</td>\n",
       "      <td>3.585938</td>\n",
       "      <td>0.311002</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3167</th>\n",
       "      <td>0.165509</td>\n",
       "      <td>0.092884</td>\n",
       "      <td>0.183044</td>\n",
       "      <td>0.070072</td>\n",
       "      <td>0.250827</td>\n",
       "      <td>0.180756</td>\n",
       "      <td>1.705029</td>\n",
       "      <td>5.769115</td>\n",
       "      <td>0.938829</td>\n",
       "      <td>0.601529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165509</td>\n",
       "      <td>0.185607</td>\n",
       "      <td>0.062257</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.227022</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3168 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      meanfreq        sd    median       Q25       Q75       IQR       skew  \\\n",
       "0     0.059781  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n",
       "1     0.066009  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n",
       "2     0.077316  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n",
       "3     0.151228  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n",
       "4     0.135120  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n",
       "...        ...       ...       ...       ...       ...       ...        ...   \n",
       "3163  0.131884  0.084734  0.153707  0.049285  0.201144  0.151859   1.762129   \n",
       "3164  0.116221  0.089221  0.076758  0.042718  0.204911  0.162193   0.693730   \n",
       "3165  0.142056  0.095798  0.183731  0.033424  0.224360  0.190936   1.876502   \n",
       "3166  0.143659  0.090628  0.184976  0.043508  0.219943  0.176435   1.591065   \n",
       "3167  0.165509  0.092884  0.183044  0.070072  0.250827  0.180756   1.705029   \n",
       "\n",
       "             kurt    sp.ent       sfm  ...  centroid   meanfun    minfun  \\\n",
       "0      274.402906  0.893369  0.491918  ...  0.059781  0.084279  0.015702   \n",
       "1      634.613855  0.892193  0.513724  ...  0.066009  0.107937  0.015826   \n",
       "2     1024.927705  0.846389  0.478905  ...  0.077316  0.098706  0.015656   \n",
       "3        4.177296  0.963322  0.727232  ...  0.151228  0.088965  0.017798   \n",
       "4        4.333713  0.971955  0.783568  ...  0.135120  0.106398  0.016931   \n",
       "...           ...       ...       ...  ...       ...       ...       ...   \n",
       "3163     6.630383  0.962934  0.763182  ...  0.131884  0.182790  0.083770   \n",
       "3164     2.503954  0.960716  0.709570  ...  0.116221  0.188980  0.034409   \n",
       "3165     6.604509  0.946854  0.654196  ...  0.142056  0.209918  0.039506   \n",
       "3166     5.388298  0.950436  0.675470  ...  0.143659  0.172375  0.034483   \n",
       "3167     5.769115  0.938829  0.601529  ...  0.165509  0.185607  0.062257   \n",
       "\n",
       "        maxfun   meandom    mindom    maxdom   dfrange   modindx  label  \n",
       "0     0.275862  0.007812  0.007812  0.007812  0.000000  0.000000      0  \n",
       "1     0.250000  0.009014  0.007812  0.054688  0.046875  0.052632      0  \n",
       "2     0.271186  0.007990  0.007812  0.015625  0.007812  0.046512      0  \n",
       "3     0.250000  0.201497  0.007812  0.562500  0.554688  0.247119      0  \n",
       "4     0.266667  0.712812  0.007812  5.484375  5.476562  0.208274      0  \n",
       "...        ...       ...       ...       ...       ...       ...    ...  \n",
       "3163  0.262295  0.832899  0.007812  4.210938  4.203125  0.161929      1  \n",
       "3164  0.275862  0.909856  0.039062  3.679688  3.640625  0.277897      1  \n",
       "3165  0.275862  0.494271  0.007812  2.937500  2.929688  0.194759      1  \n",
       "3166  0.250000  0.791360  0.007812  3.593750  3.585938  0.311002      1  \n",
       "3167  0.271186  0.227022  0.007812  0.554688  0.546875  0.350000      1  \n",
       "\n",
       "[3168 rows x 21 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
