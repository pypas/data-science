{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction\n",
    "\n",
    "## Why is Dimensionality Reduction useful?\n",
    "\n",
    "1. Data Compression\n",
    "- Reduce time complexity: less computation required\n",
    "- Reduce space complexity: less number of features\n",
    "- More interpretable: it removes noise\n",
    "\n",
    "\n",
    "2. Data Visualization\n",
    "\n",
    "3. To mitigate “the curse of dimensionality”\n",
    "\n",
    "## The curse of dimensionality\n",
    "The dotted red line shows the **optimal number of features**.\n",
    "![dim_x_perf](img/dimensionality_x_performance.png)\n",
    "\n",
    "As the dimensionality of data grows, the density of observations becomes lower and lower and lower.\n",
    "\n",
    "### Solution\n",
    "First idea: **Increase the size of the training set** to reach a sufficient density of training instances. \n",
    "\n",
    "Unfortunately, the number of training instances required to reach a given density **grows exponentially** with the\n",
    "number of dimensions. \n",
    "\n",
    "1. Feature Selection\n",
    "\n",
    "Choosing a subset of all the features (the **more informative** ones).\n",
    "\n",
    "2. Feature Extraction\n",
    "\n",
    "Create a subset of new features by **combining the existing ones**.\n",
    "\n",
    "## Principal Component Analysis (PCA)\n",
    "The most popular dimensionality reduction algorithm.\n",
    "\n",
    "PCA has two steps:\n",
    "- It identifies the hyperplane that lies closest to the data.\n",
    "- It projects the data onto it. \n",
    "\n",
    "Reduce from 2-dimension to 1-dimension: Find a direction (a vector $u^{(1)} \\in \\Re$) onto which to project the data so as to\n",
    "minimize the projection error.\n",
    "\n",
    "Reduce from n-dimension to k-dimension: Find k vectors  $u^{(1)}, u^{(2)}, ..., u^{(k)}$ onto which to project the data so as to minimize the projection error.\n",
    "\n",
    "OBS: Unlike Linear Regression, which minimizes the square error between the point and the fitted line, PCA minimizes the orthogonal distances between a point and a line. \n",
    "\n",
    "### PCA Algorithm By Eigen Decomposition\n",
    "\n",
    "1. Center the data (and normalize)\n",
    "2. Compute covariance matrix $\\Sigma$\n",
    "3. Find eigenvectors u and eigenvalues $\\lambda$\n",
    "4. Sort eigenvalues and pick first k eigenvectors\n",
    "5. Project data to k eigenvectors \n",
    "\n",
    "#### 1. Center the data\n",
    "Training set: $x^{(1)}, x^{(2)}, ..., x^{(m)}$\n",
    "\n",
    "Preprocessing (feature scaling/mean normalization):\n",
    "$\\mu_j = \\frac{1}{m}\\sum_{i=1}^m x_j^{(i)}$ --> for each feature we compute the mean of that feature\n",
    "\n",
    "Replace each $x_j^{(i)}$ with $x_j - \\mu_j$. --> this way, the feature will have 0 mean\n",
    "\n",
    "If different features on different scales, **scale features** to have comparable range of values.\n",
    "\n",
    "#### 2. Compute covariance matrix\n",
    "Reduce data from n-dimensions to k-dimensions\n",
    "\n",
    "Compute “covariance matrix”: \n",
    "\n",
    "$\\Sigma = \\frac{1}{m}\\sum_{i=1}^n(x^{(i)})(x^{(i)})^T$ --> $n x n$ matrix\n",
    "\n",
    "Covariance of $x_1$ and $x_2$: do $x_1$ and $x_2$ tend to increase together? Or does $x_2$ descrease as $x_1$ increases?\n",
    "\n",
    "Ex:\n",
    "$\\begin{bmatrix}\n",
    "2.0 & 0.8\\\\\n",
    "0.8 & 0.6 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "Eingenvectors of $\\Sigma$ are vector $u$ such as $\\Sigma u = \\lambda u$ \n",
    "\n",
    "Compute eigenvectors of $\\Sigma$: U, S, V] = svd(sigma)\n",
    "- U: the columns of u are the eigenvectors --> U we use the k first columns of this matrix \n",
    "\n",
    "$U = \\begin{pmatrix}\n",
    "\\mid & \\mid & & \\mid \\\\\n",
    "u^{(1)} & u^{(2)} & \\cdots & u^{(n)}\\\\\n",
    "\\mid & \\mid & & \\mid \\\\\n",
    "\\end{pmatrix} \\in \\Re^{n x n}$\n",
    "\n",
    "$U_{reduce} = \\begin{pmatrix}\n",
    "\\mid & \\mid & & \\mid \\\\\n",
    "u^{(1)} & u^{(2)} & \\cdots & u^{(k)}\\\\\n",
    "\\mid & \\mid & & \\mid \\\\\n",
    "\\end{pmatrix} \\in \\Re^{n x n}$\n",
    "\n",
    "$z = \\begin{pmatrix}\n",
    "\\mid & \\mid & & \\mid \\\\\n",
    "u^{(1)} & u^{(2)} & \\cdots & u^{(k)}\\\\\n",
    "\\mid & \\mid & & \\mid \\\\\n",
    "\\end{pmatrix}^T x \\in \\Re^k$\n",
    "\n",
    "\n",
    "- S = eigenvalues\n",
    "\n",
    "\n",
    "\n",
    "#### 3. Find eigenvectors u and eigenvalues $\\lambda$\n",
    "1. Find eigenvalues by solving: $det(\\Sigma - \\lambda I) = 0$\n",
    "\n",
    "$det\\begin{bmatrix}\n",
    "2.0 & 0.8\\\\\n",
    "0.8 & 0.6 \n",
    "\\end{bmatrix} = (2.0 - \\lambda)(0.6 - \\lambda)-0.8^2 = 0 \\iff {\\lambda_1, \\lambda_2}= {2.36, 0.23}$\n",
    "\n",
    "2. Find $i^{th}$ eigenvector by solvind $\\Sigma u_i = \\lambda_i u_i$\n",
    "\n",
    "3. First PC: $\\begin{bmatrix} 0.91 \\\\ 0.41 \\end{bmatrix}$, Secon PC: $\\begin{bmatrix} -0.41 \\\\ 0.91 \\end{bmatrix}$\n",
    "\n",
    "#### 4. Sort eigenvalues and pick first k eigenvectors\n",
    "If we have $n$ eingenvectors $u_1, u_2, ..., u_n$, we want $k < n$.\n",
    "\n",
    "#### 5. Project data to k eigenvectors \n",
    "\n",
    "### How to choose k\n",
    "k = number of principal  components\n",
    "\n",
    "* Average squared projection error: $\\frac{1}{m}\\sum_{i=1}^m \\lVert x^{(i)} - x_{approx}^{(i)} \\rVert^2$\n",
    "\n",
    "Difference between the original data $x$ and the projected version $x_approx$. PCA tries to minimize that value.\n",
    "\n",
    "* Total variation in the data: $\\frac{1}{m}\\sum_{i=1}^m \\lVert x^{(i)} \\rVert^2$ \n",
    "\n",
    "On average, how far are my training examples from the vector (from just being all zeros)\n",
    "\n",
    "Typically, chose k to be the smallest value so that\n",
    "$\\frac{\\frac{1}{m}\\sum_{i=1}^m \\lVert x^{(i)} - x_{approx}^{(i)} \\rVert^2}{\\frac{1}{m}\\sum_{i=1}^m \\lVert x^{(i)} \\rVert^2} \\leq 0.01$\n",
    "\n",
    "\"99% of variance is retained\"\n",
    "\n",
    "$\\frac{\\frac{1}{m}\\sum_{i=1}^m \\lVert x^{(i)} - x_{approx}^{(i)} \\rVert^2}{\\frac{1}{m}\\sum_{i=1}^m \\lVert x^{(i)} \\rVert^2} = 1 - \\frac{\\sum_{i=1}^k S_{ii}}{\\sum_{i=1}^n S_{ii}}$\n",
    "\n",
    "### Using PCA to speed up the raining time of a learning algorithm\n",
    "Supervised learning algorithm with: $(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(m)}, y^{(m)})$\n",
    "\n",
    "1. Extract inputs\n",
    "\n",
    "$x^{(1)},  x^{(2)}, ..., x^{(m)} \\in \\Re^{10000}$\n",
    "\n",
    "2. Apply PCA to inputs\n",
    "\n",
    "$z^{(1)},  z^{(2)}, ..., z^{(m)} \\in \\Re^{1000}$\n",
    "\n",
    "3. New dataset\n",
    "\n",
    "$(z^{(1)}, y^{(1)}), (z^{(2)}, y^{(2)}), ..., (z^{(m)}, y^{(m)})$\n",
    "\n",
    "4. Apply the algorithm\n",
    "\n",
    "*OBS*: Mapping $x^{(i)} \\to z^{(i)}$ should be defined by running PCA only on the training set.\n",
    "\n",
    "### Bad use of PCA\n",
    "* To prevent overfitting\n",
    "\n",
    "This might work OK, but it's not a good way to address overfitting. Use **regularization** instead\n",
    "\n",
    "* Design of ML system\n",
    "\n",
    "Before implementing PCA, first try running whatever you want to fo with the original/raw data. Only if that doesn't work well, then implement PCA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
