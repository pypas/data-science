{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "Src: http://neuralnetworksanddeeplearning.com/chap1.html\n",
    "\n",
    "## Why Now?\n",
    "Scale drives deep learning progress:\n",
    "- Data\n",
    "- Computation\n",
    "- Algorithms\n",
    "\n",
    "## One-layer neural network\n",
    "$\\hat{y} = \\sigma(w^Tx + b)$, where $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "Given ${(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(m)}, y^{(m)})}$ we want $\\hat{y}^{(i)} \\approx y^{(i)}$\n",
    "\n",
    "### Loss function\n",
    "\"How well we are doing in a single training example?\"\n",
    "\n",
    "First idea: $L(\\hat{y}, y) = \\frac{1}{2}(\\hat{y}-y)^2$. However, we don't usually do this because in the process of learning the parameters, the optimization problem  becomes non-convex.\n",
    "\n",
    "We use instead: $L(\\hat{y},y) = -(ylog(\\hat{y}) + (1-y)log(1-\\hat{y}))$.\n",
    "\n",
    "If $y=1$, the loss function becomes $L(\\hat{y},y) = -log(\\hat{y})$.\n",
    "- $\\hat{y} = 1$: $L(\\hat{y},y) = 0$\n",
    "- $\\hat{y} \\approx 0$: $L(\\hat{y},y) = very large$\n",
    "\n",
    "### Cost function\n",
    "\"How well we are doing in the whole training set?\"\n",
    "\n",
    "$J(w,b) = \\frac{1}{m}\\sum_{i=1}^mL(\\hat{y}^{(i)},y^{(i)})$\n",
    "\n",
    "## Gradient Descent\n",
    "J is a convex function. Any local minimum of a convex function is also a global minimum. A strictly convex function will have at most one global minimum.\n",
    "\n",
    "repeat {\n",
    "    $w: w - \\alpha\\frac{dJ(w)}{dw}$\n",
    "}\n",
    "\n",
    "$L(a,y) = -(ylog(a) + (1-y)log(1-a))$, where $a = \\sigma(z)$\n",
    "\n",
    "$\\frac{dL(a,y)}{da} = -\\frac{y}{a} + \\frac{1-y}{1-a}$\n",
    "\n",
    "$\\frac{dL(a,y)}{dz} = (-\\frac{y}{a} + \\frac{1-y}{1-a}) * \\frac{da}{dz}$, and we have that:\n",
    "\n",
    "$\\frac{da}{dz} = \\frac{d}{dz}\\frac{1}{1+e^{-z}} = \\frac{e^{-z}}{(1+e^{-z})^{2}} = \\frac{1 + e^{-z} - 1}{(1+e^{-z})^{2}} = \\frac{1}{1 + e^{-z}}(1 - \\frac{1}{1 + e^{-z}}) = a(1 - a)$\n",
    "\n",
    "So: $\\frac{dL(a,y)}{dz} = (-\\frac{y}{a} + \\frac{1-y}{1-a}) * a(1-a) = a - y$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron\n",
    "A perceptron takes several binary inputs, $x_1,x_2,...$, and produces a single binary output. \n",
    "\n",
    "To compute the output, we use the concept of **weights**, $w_1,w_2,...$, real numbers expressing the **importance** of the respective inputs to the output.\n",
    "\n",
    "The neuron's output, 0 or 1, is determined by whether the weighted sum $\\sum_jw_jx_j$ is less than or greater than some threshold value:\n",
    "\n",
    "$output = \\left \\{ \\begin{matrix} 0, & \\mbox{if }\\sum_jw_jx_j \\leq threshold \\\\ 1, & \\mbox{if }\\sum_jw_jx_j \\gt threshold \\end{matrix} \\right.$\n",
    "\n",
    "Now, instead of writing $\\sum_jw_jx_j$, we will simply write $w.x$. Also, we will move the threshold to the other side of the equation so that $b = -threshold$.\n",
    "\n",
    "$output = \\left \\{ \\begin{matrix} 0, & \\mbox{if } w.x + b \\leq 0 \\\\ 1, & \\mbox{if }w.x + b \\gt 0 \\end{matrix} \\right.$\n",
    "\n",
    "You can think of the bias as a measure of **how easy it is to get the perceptron to output a 1**.\n",
    "\n",
    "If the bias is very negative, the threshold is very high, which means it's difficult for the perceptron to output a 1.\n",
    "\n",
    "### Input Layer\n",
    "\n",
    "First layer, that encodes the inputs. We can think of the input perceptrons as not really being perceptrons at all, but rather special units which are simply defined to output the desired values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Neurons\n",
    "### How to train a network?\n",
    "What we want is that when we make a small change in the weights, we can observe a small change in the output. Then, we can use this fact to modify the weights and bias in order to make the network better.\n",
    "\n",
    "However, if the network contains perceptrons, a small change in the weights or bias of any perceptron can sometimes cause the output of that perceptor to completely flip, from 0 to 1 or from 1 to 0. \n",
    "\n",
    "### What is a Sigmoid neuron?\n",
    "Sigmoid neurons are similar to perceptrons, but modified so that small changes in their weights and bias cause only a small change in their output. Now, instead of binary inputs, we will have continuous inputs between 0 and 1. The output is now $\\sigma(w.x +b)$, where \n",
    "\n",
    "$\\sigma(z)=\\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "If $z$ is high, the output will be approximately 1. If $z$ is very negative, the output will be close to 0.\n",
    "\n",
    "If instead of a sigmoid funcion we had used a step function, that neuron would be a perceptron.\n",
    "\n",
    "### Calculating $\\Delta$ouput\n",
    "\n",
    "$\\Delta output \\approx \\sum_j\\frac{\\delta output}{\\delta w_j}\\Delta w_j + \\frac{\\delta output}{\\delta b}\\Delta b$\n",
    "\n",
    "That means that $\\Delta output$ is a linear function on $\\Delta w_j$ and $\\Delta b$, that is, the relationship between the changes in weight/bias and the impact on output change is easier to understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "### Step function\n",
    "Problems:\n",
    "\n",
    "- Binary classifier (“yes” or “no”, activate or not activate): a step function could do that for you!\n",
    "\n",
    "- Multi classifier (class1, class2, class3, etc). What will happen if more than 1 neuron is “activated”?\n",
    "\n",
    "### Sigmoid function\n",
    "$\\sigma(z)=\\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "Advantages:\n",
    "- The output of the activation function is always going to be in range (0,1).\n",
    "- It is nonlinear in nature.\n",
    "- Combinations of this function are also nonlinear! Great!!\n",
    "\n",
    "Problems:\n",
    "- Towards either end of the sigmoid function, the $\\sigma(x)$ values tend to respond much less to changes in x. \n",
    "\n",
    "- The problem of “vanishing gradients”: Cannot make significant change because of the extremely small value\n",
    "\n",
    "### Tanh function\n",
    "$tanh(x) = \\frac{2}{1 - e^{-2x}} - 1$\n",
    "\n",
    "Advantages:\n",
    "- The output of the activation function is always going to be in range (-1,1).\n",
    "- It is nonlinear in nature.\n",
    "- Combinations of this function are also nonlinear! Great!!\n",
    "\n",
    "Problems:\n",
    "- Also has the \"vanishing gradients\" problems\n",
    "\n",
    "### ReLu function\n",
    "$ReLU(x) = max(0,x)$\n",
    "\n",
    "If you don’t know the nature of the function you are trying to learn, start with ReLU.\n",
    "\n",
    "Advantages:\n",
    "- It gives an output x if x is positive and 0 otherwise. The range is (0, inf).\n",
    "- It is nonlinear in nature. Combinations of this function are also nonlinear!\n",
    "- Sparsity of the activation!\n",
    "\n",
    "Problems:\n",
    "- Because of the horizontal line in ReLU( for negative x ), the gradient can go towards 0.\n",
    "- “Dying ReLU problem”: several neurons can just die and not respond making a substantial part of the network passive.\n",
    "\n",
    "### Leaky ReLu function\n",
    "$ReLu(x) = \\left \\{ \\begin{matrix} x, & \\mbox{if } x > 0 \\\\ 0.01x, & \\mbox{otherwise }  \\end{matrix} \\right.$\n",
    "                    \n",
    "\n",
    "Advantages:\n",
    "- It gives an output x if x is positive and 0 otherwise. The range is (0, inf).\n",
    "- (Leaky) ReLU is less computationally expensive than tanh and sigmoid because it involves simpler mathematical operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "Layer 1 = Input layer\n",
    "- $x_1, x_2, x_3$ : inputs\n",
    "- $x_0$ : bias\n",
    "\n",
    "Layer 2 = Hidden layer\n",
    "- $a_1^{(2)}, a_2^{(2)}, a_3^{(2)}$ : activation of unit i in layer 2\n",
    "\n",
    "Layer 3 = Output layer\n",
    "- $a_1^{(3)}$\n",
    "\n",
    "### Weights\n",
    "$\\Theta_j$: matrix of weights from layer j to layer j+1\n",
    "\n",
    "### Forward Propagating\n",
    "$a_1^{(2)} = g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2 + \\Theta_{13}^{(1)}x_3)$\n",
    "\n",
    "$a_2^{(2)} = g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2 + \\Theta_{23}^{(1)}x_3)$\n",
    "\n",
    "$a_3^{(2)} = g(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2 + \\Theta_{33}^{(1)}x_3)$\n",
    "\n",
    "$h_{\\Theta(x)} = a_1^{(3)} = g(\\Theta_{10}^{(2)}a_0^{(2)} + \\Theta_{11}^{(2)}a_1^{(2)} + \\Theta_{12}^{(2)}a_2^{(2)} + \\Theta_{13}^{(2)}a_3^{(2)})$\n",
    "\n",
    "### Selecting the Architecture\n",
    "- Input units: dimensionality of the problem (features x)\n",
    "- Output units: Number of classes\n",
    "- Hidden units (per layer):\n",
    "    - Usually, the more the better \n",
    "    - Good start: a number **close to the number of inputs**\n",
    "    - Default: 1 hidden layer. If you have >1 hidden layer, then it is interesting that you have the **same number of units in every hidden layer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Neural Network\n",
    "### The training process\n",
    "1. Initialize the parameters\n",
    "\n",
    "2. Choose an optimization algorithm\n",
    "\n",
    "3. Repeat these steps:\n",
    "    - Forward propagate an input\n",
    "    - Compute the cost function\n",
    "    - Compute the gradients of the cost with respect to parameters using backpropagation\n",
    "    - Update each parameter using the gradients, according to the optimization algorithm\n",
    "\n",
    "\n",
    "### Step 1 - Random Initialization\n",
    "Src: https://www.deeplearning.ai/ai-notes/initialization/\n",
    "\n",
    "The first step of training a network is initializing the weights. \n",
    "\n",
    "- Initializing all the weights **with zeros** leads the neurons to **learn the same features** during training.\n",
    "\n",
    "\"Consider a neural network with two hidden units, and assume we initialize all the biases to 0 and the weights with some constant $\\alpha$. If we forward propagate an input $(x_1,x_2)$ in this network, the output of both hidden units will be $relu(\\alpha x_1 + \\alpha x_2)$. Thus, both hidden units will have identical influence on the cost, which will lead to identical gradients. Thus, **both neurons will evolve symmetrically** throughout training, effectively preventing different neurons from learning different things.\"\n",
    "\n",
    "- Initializing the weights with values that are too small leads to slow learning, and values that are too big lead to divergence. (read: vanishing/exploding gradients)\n",
    "\n",
    "#### Rules of thumb\n",
    "- The mean of the activations should be zero.\n",
    "- The variance of the activations should stay the same across every layer.\n",
    "\n",
    "Considering a layer $l$, the forward propagation equations can be written as:\n",
    "$a^{(l-1)} = g^{[l-1]}(z^{[l-1]})$\n",
    "\n",
    "$z^{[l]} = W^{[l]}*a^{[l-1]} + b^{[l]}$\n",
    "\n",
    "$a^{(l)} = g^{[l]}(z^{[l]})$\n",
    "\n",
    "What we want is: $E[a^{[l-1]}] = E[a^{[l]}]$ and $var[a^{[l-1]}] = var[a^{[l]}]$\n",
    "\n",
    "\n",
    "#### Xavier initialization\n",
    "$W^{[l]} \\approx N(\\mu = 0, \\sigma^2 = \\frac{1}{n^{[l-1]}})$, where $n^{[l-1]}$ is the number of neurons in layer $l-1$\n",
    "\n",
    "$b^{[l]} = 0$\n",
    "\n",
    "### Step 2  - Feed Forward\n",
    "Now, we need to forward propagate our inputs in order to get the outputs\n",
    "\n",
    "` def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if \"a\" is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a`\n",
    "        \n",
    "Given one training example $(x,y)$:\n",
    "Forward propagation:\n",
    "$a^{(1)} = x$\n",
    "\n",
    "$z^{(2)} = \\Theta^{(1)}a^{(1)}$\n",
    "\n",
    "$a^{(2)} = g(z^{(2)})$ (add bias $a_0^{(2)}$)\n",
    "\n",
    "$z^{(3)} = \\Theta^{(2)}a^{(2)}$\n",
    "\n",
    "$a^{(3)} = g(z^{(3)})$ (add bias $a_0^{(3)}$)\n",
    "\n",
    "$z^{(4)} = \\Theta^{(3)}a^{(3)}$\n",
    "\n",
    "$a^{(4)} = g(z^{(4)}) = h_{\\Theta}(x)$ \n",
    "\n",
    "### Step 3 - Calculate Loss Function\n",
    "We compare our outputs obtained in the previous step to the desidered outputs, calculating the loss function.\n",
    "\n",
    "### Step 4 - Calculate the Derivative of the Error\n",
    "$\\delta_j^{(l)}$ = error of node $j$ in layer $l$.\n",
    "\n",
    "For each output unit (layer):\n",
    "$\\delta_j^{(4)} = a_j^{(4)} - y_j$ (hypothesis output - real value of y)\n",
    "\n",
    "$\\delta^{(4)} = a^{(4)} - y$\n",
    "\n",
    "### Step 5 - Backpropagate\n",
    "For each hidden unit:\n",
    "\n",
    "$\\delta^{(3)} = (\\Theta^{(3)})^T\\delta^{(4)}.*g'(z^{(3)})$\n",
    "\n",
    "$\\delta^{(2)} = (\\Theta^{(2)})^T\\delta^{(3)}.*g'(z^{(2)})$\n",
    "\n",
    "OBS: $g'(z) = g(z)(1 - g(z))$, and therefore $g'(z^{(3)}) = a^{(3)}.*(1-a^{(3)})$\n",
    "\n",
    "We can show that $\\frac{\\delta}{\\delta\\Theta_{ij}^{(l)}}J(\\Theta) = a_j^{(l)}\\delta_i^{(l+1)}$\n",
    "\n",
    "#### Backpropagation algorithm\n",
    "Given training set $(x^{(1)}, y^{(1)}) ... (x^{(m)}, y^{(m)})$.\n",
    "\n",
    "- Set $\\Delta^{(l)}_{i,j} = 0$ for all (l,i,j), (hence you end up having a matrix full of zeros)\n",
    "\n",
    "For training example $t = 1$ to $m$:\n",
    "1. Set $a^{(1)} := x^{(t)}$\n",
    "\n",
    "2. Perform forward propagation to compute $a^{(l)}$ for $l=2,3,…,L$\n",
    "\n",
    "3. Using $y^{(t)}$, compute $\\delta^{(L)} = a^{(L)} - y^{(t)}$\n",
    "\n",
    "4. Compute $\\delta^{(L-1)}, \\delta^{(L-2)},...,\\delta^{(2)}$ using $\\delta^{(l)} = ((\\Theta^{(l)})^T \\delta^{(l+1)})\\ .*\\ a^{(l)}\\ .*\\ (1 - a^{(l)})$\n",
    "\n",
    "5. $\\Delta_{i,j}^{(l)}= \\Delta_{i,j}^{(l)} + a_j^{(l)}\\delta_i^{(l+1)}$ or with vectorization, $\\Delta^{(l)} := \\Delta^{(l)} + \\delta^{(l+1)}(a^{(l)})^T$\n",
    "\n",
    "### How many iterations to converge?\n",
    "1. It depends on the meta-parameters of the network (how many layers, how complex the nonlinear functions are)\n",
    "2. It depends on the learning rate. \n",
    "3. It depends on the optimization method\n",
    "4. It depends on the random initialization of the network. \n",
    "5. It depends on the quality of the training set. \n",
    "\n",
    "https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "\n",
    "### Softmax classification\n",
    "TODO\n",
    "\n",
    "https://www.freecodecamp.org/news/building-a-neural-network-from-scratch/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
